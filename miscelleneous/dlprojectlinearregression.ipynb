{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrashantTiwari\\AppData\\Local\\Temp\\ipykernel_9620\\2290466576.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column] = label_encoders[column].fit_transform(X[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 307.23769286690396\n",
      "Test RMSE: 303.92076241926225\n",
      "Train R^2 score: 0.15266602724488898\n",
      "Test R^2 score: 0.019716829445878825\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"artDataset.csv\")\n",
    "\n",
    "# Assuming 'Price' is the target variable\n",
    "X = df[['artist', 'title', 'signed', 'condition', 'period', 'movement']]  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Encoding categorical variables if needed\n",
    "label_encoders = {}\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        X[column] = label_encoders[column].fit_transform(X[column])\n",
    "\n",
    "# Convert target variable to numeric format\n",
    "y = y.str.replace(' USD', '').astype(float)  # Remove ' USD' and convert to float\n",
    "\n",
    "# Splitting the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "train_rmse = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Train R^2 score:\", r2_train)\n",
    "print(\"Test R^2 score:\", r2_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames.\n",
      "Found 0 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:1137: UserWarning: Found 754 invalid image filename(s) in x_col=\"Image_Path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Extract images from zip file\n",
    "with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('art_dataset_extracted')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"artDataset.csv\")\n",
    "\n",
    "# Assuming 'Price' is the target variable\n",
    "X_textual = df[['artist', 'title', 'signed', 'condition', 'period', 'movement']].values  # Textual features\n",
    "X_numeric = df.select_dtypes(include=np.number).values  # Numerical features\n",
    "y = df['price'].values  # Target variable\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include='object').columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train_textual, X_test_textual, X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(\n",
    "    X_textual, X_numeric, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Image data generator\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Define the image directory\n",
    "image_directory = \"art_dataset_extracted/imgDataset\"  # Path to the directory containing images\n",
    "\n",
    "# Generate file paths for images\n",
    "# Generate file paths for images\n",
    "df['Image_Path'] = 'image_' + (df.index + 1).astype(str) + '.png'  # Assuming image file extensions are '.jpg'\n",
    "\n",
    "# Splitting the dataset into train and validation sets\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=image_directory,\n",
    "    x_col='Image_Path',  # Column containing the file paths of the images\n",
    "    y_col=\"price\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=image_directory,\n",
    "    x_col='Image_Path',  # Column containing the file paths of the images\n",
    "    y_col=\"price\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# CNN model\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(image_input)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flatten = Flatten()(pool2)\n",
    "\n",
    "# Numerical and textual input\n",
    "numeric_input = Input(shape=(X_numeric.shape[1],))\n",
    "textual_input = Input(shape=(X_textual.shape[1],))\n",
    "\n",
    "# Combine all inputs\n",
    "combined_input = Concatenate()([flatten, numeric_input, textual_input])\n",
    "\n",
    "# Fully connected layers\n",
    "dense1 = Dense(128, activation='relu')(combined_input)\n",
    "output = Dense(1)(dense1)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[image_input, numeric_input, textual_input], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
    "\n",
    "# # Train model\n",
    "# history = model.fit(\n",
    "#     [train_generator, X_train_numeric, X_train_textual],\n",
    "#     y_train,\n",
    "#     validation_data=([validation_generator, X_test_numeric, X_test_textual], y_test),\n",
    "#     epochs=10,\n",
    "#     batch_size=32\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames.\n",
      "Found 0 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\keras\\src\\preprocessing\\image.py:1137: UserWarning: Found 754 invalid image filename(s) in x_col=\"Image_Path\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[RootMeanSquaredError()])\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_textual\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_textual\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[0;32m    101\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Extract images from zip file\n",
    "with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('art_dataset_extracted')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"artDataset.csv\")\n",
    "\n",
    "# Assuming 'Price' is the target variable\n",
    "X_textual = df[['artist', 'title', 'signed', 'condition', 'period', 'movement']].values  # Textual features\n",
    "X_numeric = df.select_dtypes(include=np.number).values  # Numerical features\n",
    "y = df['price'].values  # Target variable\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoders = {}\n",
    "for column in df.select_dtypes(include='object').columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train_textual, X_test_textual, X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(\n",
    "    X_textual, X_numeric, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Image data generator\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Define the image directory\n",
    "image_directory = \"art_dataset_extracted/imgDataset\"  # Path to the directory containing images\n",
    "\n",
    "# Generate file paths for images\n",
    "df['Image_Path'] = 'image_' + (df.index + 1).astype(str) + '.jpg'  # Assuming image file extensions are '.jpg'\n",
    "\n",
    "# Splitting the dataset into train and validation sets\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=image_directory,\n",
    "    x_col='Image_Path',  # Column containing the file paths of the images\n",
    "    y_col=\"price\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=image_directory,\n",
    "    x_col='Image_Path',  # Column containing the file paths of the images\n",
    "    y_col=\"price\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# CNN model\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(image_input)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flatten = Flatten()(pool2)\n",
    "\n",
    "# Numerical and textual input\n",
    "numeric_input = Input(shape=(X_numeric.shape[1],))\n",
    "textual_input = Input(shape=(X_textual.shape[1],))\n",
    "\n",
    "# Combine all inputs\n",
    "combined_input = Concatenate()([flatten, numeric_input, textual_input])\n",
    "\n",
    "# Fully connected layers\n",
    "dense1 = Dense(128, activation='relu')(combined_input)\n",
    "output = Dense(1)(dense1)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[image_input, numeric_input, textual_input], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    [train_generator.next()[0], X_train_numeric, X_train_textual],\n",
    "    train_generator.next()[1],\n",
    "    validation_data=([validation_generator.next()[0], X_test_numeric, X_test_textual], validation_generator.next()[1]),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrashantTiwari\\AppData\\Local\\Temp\\ipykernel_908\\1303691931.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column] = label_encoders[column].fit_transform(X[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "19/19 [==============================] - 3s 3ms/step - loss: 152692.5625\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 152604.9531\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 152484.3906\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 152321.9844\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 152097.7188\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 151796.4375\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 151438.0781\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 150999.4219\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 150531.6250\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 150010.1406\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 149488.2969\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 148944.5469\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 148395.6406\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 147869.8438\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 147344.6406\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 146811.7656\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 146322.8906\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 145828.3750\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 145360.9219\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 144896.5469\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 144441.2031\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 144011.0625\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 143584.0938\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 143173.8438\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 142775.4844\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 142384.0000\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 141998.6406\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 141632.2969\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 141261.5312\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 140910.1406\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 140564.5469\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 140224.5156\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 139895.5312\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 139567.0625\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 139240.7969\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 138935.8125\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 138624.9219\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 138320.9531\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 138018.5000\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 137734.9844\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 137434.0938\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 137162.4375\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 136878.8438\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 136604.4062\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 136332.2500\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 136075.2812\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 135800.8906\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 135552.2812\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 135293.3750\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 135044.9844\n",
      "19/19 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 0s/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (603,1) doesn't match the broadcast shape (603,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Inverse scaling for predicted values\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_test)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:461\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    456\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    458\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    459\u001b[0m                 force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 461\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[0;32m    462\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (603,1) doesn't match the broadcast shape (603,6)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"artDataset.csv\")\n",
    "\n",
    "# Assuming 'Price' is the target variable\n",
    "X = df[['artist', 'title', 'signed', 'condition', 'period', 'movement']]  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Encoding categorical variables if needed\n",
    "label_encoders = {}\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        X[column] = label_encoders[column].fit_transform(X[column])\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert target variable to numpy array\n",
    "y = y.str.replace(' USD', '').astype(float).values  # Remove ' USD' and convert to float\n",
    "\n",
    "# Splitting the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input data for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Inverse scaling for predicted values\n",
    "y_pred_train = scaler.inverse_transform(y_pred_train)\n",
    "y_pred_test = scaler.inverse_transform(y_pred_test)\n",
    "\n",
    "# Evaluation\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Train R^2 score:\", r2_train)\n",
    "print(\"Test R^2 score:\", r2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PrashantTiwari\\AppData\\Local\\Temp\\ipykernel_908\\1764822874.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column] = label_encoders[column].fit_transform(X[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 2s 5ms/step - loss: 152655.0625\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 152350.2812\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 151153.3750\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 149190.5156\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 148034.5000\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 147406.4219\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 146899.1406\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 146436.6094\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 146003.8438\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 145607.5156\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 145202.5312\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 144817.8750\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 144460.2500\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 144075.9062\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 143721.6875\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 143364.7188\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 143029.1406\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 142678.6406\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 142359.2031\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 142016.8125\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 141688.3594\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 141378.3438\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 141052.2812\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 140732.7969\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 140426.8281\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 140112.4219\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 139827.7344\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 139505.8594\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 139220.3750\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 138920.2500\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 138631.6875\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 138340.9688\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 138051.4062\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 137769.9219\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 137488.1250\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 137202.1094\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 136930.9688\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 136654.1094\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 136385.4531\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 136114.5938\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 135851.0781\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 135584.3594\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 135318.9062\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 135061.1562\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 134806.0312\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 134550.7969\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 134294.9688\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 134046.1719\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 133798.7188\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 133557.5000\n",
      "19/19 [==============================] - 1s 2ms/step\n",
      "5/5 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (603,1) doesn't match the broadcast shape (603,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Inverse scaling for predicted values\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_test)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PrashantTiwari\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:461\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    456\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    458\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    459\u001b[0m                 force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 461\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[0;32m    462\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (603,1) doesn't match the broadcast shape (603,6)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"artDataset.csv\")\n",
    "\n",
    "# Assuming 'Price' is the target variable\n",
    "X = df[['artist', 'title', 'signed', 'condition', 'period', 'movement']]  # Features\n",
    "y = df['price']  # Target variable\n",
    "\n",
    "# Encoding categorical variables if needed\n",
    "label_encoders = {}\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        label_encoders[column] = LabelEncoder()\n",
    "        X[column] = label_encoders[column].fit_transform(X[column])\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert target variable to numpy array\n",
    "y = y.str.replace(' USD', '').astype(float).values  # Remove ' USD' and convert to float\n",
    "\n",
    "# Splitting the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input data for LSTM\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(units=1))  # Adjusted output units to 1\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Inverse scaling for predicted values\n",
    "y_pred_train = scaler.inverse_transform(y_pred_train)\n",
    "y_pred_test = scaler.inverse_transform(y_pred_test)\n",
    "\n",
    "# Evaluation\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Train R^2 score:\", r2_train)\n",
    "print(\"Test R^2 score:\", r2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiod\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovement\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myearCreation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     22\u001b[0m encoded_data \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(data[categorical_columns])\n\u001b[1;32m---> 23\u001b[0m encoded_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(encoded_data\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m())\n\u001b[0;32m     24\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data\u001b[38;5;241m.\u001b[39mdrop(categorical_columns, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), encoded_df], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# # Split the Data into Training and Testing Sets\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# X = data.drop('price', axis=1)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# y = data['price']\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# print(\"Train R^2 score:\", r2_train)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# print(\"Test R^2 score:\", r2_test)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('artDataset.csv')\n",
    "\n",
    "# Preprocess the Data\n",
    "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "data['price'] = data['price'].str.extract('(\\d+\\.\\d+)')[0].astype(float)\n",
    "data['artist'].fillna('Unknown Artist', inplace=True)\n",
    "data['yearCreation'] = data['yearCreation'].str.extract('(\\d{4})').fillna('Unknown')\n",
    "\n",
    "# Encode Categorical Variables\n",
    "encoder = OneHotEncoder()\n",
    "categorical_columns = ['artist', 'title', 'signed', 'condition', 'period', 'movement', 'yearCreation']\n",
    "encoded_data = encoder.fit_transform(data[categorical_columns])\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out())\n",
    "data = pd.concat([data.drop(categorical_columns, axis=1), encoded_df], axis=1)\n",
    "\n",
    "# Split the Data into Training and Testing Sets\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input data for LSTM\n",
    "X_train = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Train R^2 score:\", r2_train)\n",
    "print(\"Test R^2 score:\", r2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
